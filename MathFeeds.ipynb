{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from lxml import etree\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, core\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import gensim\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xml2df(xml_data):\n",
    "    tree = ET.parse(xml_data)\n",
    "    root = tree.getroot()\n",
    "    all_records = []\n",
    "    headers = []\n",
    "    for i, child in enumerate(root):\n",
    "        record = []\n",
    "        for subchild in child:\n",
    "            record.append(subchild.text)\n",
    "            if subchild.tag not in headers:\n",
    "                headers.append(subchild.tag)\n",
    "        all_records.append(record)\n",
    "    return pd.DataFrame(all_records, columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = xml2df(\"MathFeedsDataAll.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Domain</th>\n",
       "      <th>blurb</th>\n",
       "      <th>date</th>\n",
       "      <th>image</th>\n",
       "      <th>isbn</th>\n",
       "      <th>kicker</th>\n",
       "      <th>price</th>\n",
       "      <th>timesDeleted</th>\n",
       "      <th>timesEmailed</th>\n",
       "      <th>timesOpened</th>\n",
       "      <th>timesSaved</th>\n",
       "      <th>timesShared</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>wordtitle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://blogs.ams.org/mathgradblog/2017/01/05/d...</td>\n",
       "      <td>Up to Date Blog Content for JMM 2017</td>\n",
       "      <td>Looking for blog content about the 2017 Joint ...</td>\n",
       "      <td>blogs.ams.org</td>\n",
       "      <td>Looking for blog content about the 2017 Joint ...</td>\n",
       "      <td>01/05/17</td>\n",
       "      <td>None</td>\n",
       "      <td>nan</td>\n",
       "      <td>GRADUATE STUDENT BLOG</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>4.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>5774489532.046694</td>\n",
       "      <td>uptodateblogcontentforjmm2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mrhonner.com/archives/17215</td>\n",
       "      <td>Math Photo: A Dodecagon of Octagons « Mr Honner</td>\n",
       "      <td>I’d never looked closely at the Parachute Jump...</td>\n",
       "      <td>mrhonner.com</td>\n",
       "      <td>I'd never looked closely at the Parachute Jump...</td>\n",
       "      <td>09/18/16</td>\n",
       "      <td>http://MrHonner.com/wp-content/uploads/2016/09...</td>\n",
       "      <td>nan</td>\n",
       "      <td>NOTABLE</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>5783496448.673001</td>\n",
       "      <td>mathphotoadodecagonofoctagons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://mathbabe.org/2017/03/21/guest-post-the...</td>\n",
       "      <td>Guest post: the age of algorithms | mathbabe</td>\n",
       "      <td>Artie has kindly allowed me to post his though...</td>\n",
       "      <td>mathbabe.org</td>\n",
       "      <td>Artie has kindly allowed me to post his though...</td>\n",
       "      <td>03/21/17</td>\n",
       "      <td>None</td>\n",
       "      <td>nan</td>\n",
       "      <td>BLOG</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>5767987618.7198305</td>\n",
       "      <td>guestposttheageofalgorithms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.scientificamerican.com/podcast/epis...</td>\n",
       "      <td>Sean M. Carroll Looks at The Big Picture - Sci...</td>\n",
       "      <td>Steve Mirsky: Welcome to Scientific American's...</td>\n",
       "      <td>scientificamerican.com</td>\n",
       "      <td>Caltech theoretical physicist Sean M. Carroll ...</td>\n",
       "      <td>05/12/16</td>\n",
       "      <td>https://www.scientificamerican.com/sciam/cache...</td>\n",
       "      <td>nan</td>\n",
       "      <td>None</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>5795107200.0</td>\n",
       "      <td>seanmcarrolllooksatthebigpicture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://the-japan-news.com/news/article/0003176002</td>\n",
       "      <td>The Japan News</td>\n",
       "      <td>Not found\\n\\nThe requested server cannot be ac...</td>\n",
       "      <td>the-japan-news.com</td>\n",
       "      <td>The education ministry will open research cent...</td>\n",
       "      <td>09/13/16</td>\n",
       "      <td>http://the-japan-news.com/modules/img/logo_ogp...</td>\n",
       "      <td>nan</td>\n",
       "      <td>BIG DATA</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>5784392943.842338</td>\n",
       "      <td>educationministrytopromoteuseofbigdata</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL  \\\n",
       "0  http://blogs.ams.org/mathgradblog/2017/01/05/d...   \n",
       "1                 http://mrhonner.com/archives/17215   \n",
       "2  https://mathbabe.org/2017/03/21/guest-post-the...   \n",
       "3  http://www.scientificamerican.com/podcast/epis...   \n",
       "4  http://the-japan-news.com/news/article/0003176002   \n",
       "\n",
       "                                               Title  \\\n",
       "0               Up to Date Blog Content for JMM 2017   \n",
       "1    Math Photo: A Dodecagon of Octagons « Mr Honner   \n",
       "2       Guest post: the age of algorithms | mathbabe   \n",
       "3  Sean M. Carroll Looks at The Big Picture - Sci...   \n",
       "4                                     The Japan News   \n",
       "\n",
       "                                                Text                  Domain  \\\n",
       "0  Looking for blog content about the 2017 Joint ...           blogs.ams.org   \n",
       "1  I’d never looked closely at the Parachute Jump...            mrhonner.com   \n",
       "2  Artie has kindly allowed me to post his though...            mathbabe.org   \n",
       "3  Steve Mirsky: Welcome to Scientific American's...  scientificamerican.com   \n",
       "4  Not found\\n\\nThe requested server cannot be ac...      the-japan-news.com   \n",
       "\n",
       "                                               blurb      date  \\\n",
       "0  Looking for blog content about the 2017 Joint ...  01/05/17   \n",
       "1  I'd never looked closely at the Parachute Jump...  09/18/16   \n",
       "2  Artie has kindly allowed me to post his though...  03/21/17   \n",
       "3  Caltech theoretical physicist Sean M. Carroll ...  05/12/16   \n",
       "4  The education ministry will open research cent...  09/13/16   \n",
       "\n",
       "                                               image isbn  \\\n",
       "0                                               None  nan   \n",
       "1  http://MrHonner.com/wp-content/uploads/2016/09...  nan   \n",
       "2                                               None  nan   \n",
       "3  https://www.scientificamerican.com/sciam/cache...  nan   \n",
       "4  http://the-japan-news.com/modules/img/logo_ogp...  nan   \n",
       "\n",
       "                  kicker price timesDeleted timesEmailed timesOpened  \\\n",
       "0  GRADUATE STUDENT BLOG   nan          1.0          nan         4.0   \n",
       "1                NOTABLE   nan          nan          nan         nan   \n",
       "2                   BLOG   nan          2.0          2.0        11.0   \n",
       "3                   None   nan          nan          nan         nan   \n",
       "4               BIG DATA   nan          nan          nan         nan   \n",
       "\n",
       "  timesSaved timesShared           timestamp  \\\n",
       "0        nan         nan   5774489532.046694   \n",
       "1        nan         nan   5783496448.673001   \n",
       "2        1.0         nan  5767987618.7198305   \n",
       "3        nan         nan        5795107200.0   \n",
       "4        nan         nan   5784392943.842338   \n",
       "\n",
       "                                wordtitle  \n",
       "0           uptodateblogcontentforjmm2017  \n",
       "1           mathphotoadodecagonofoctagons  \n",
       "2             guestposttheageofalgorithms  \n",
       "3        seanmcarrolllooksatthebigpicture  \n",
       "4  educationministrytopromoteuseofbigdata  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "#df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_word(w):\n",
    "    strip_str = \"()\\\".?!,;\"\n",
    "    new_word = \"\".join((c for c in w if c in string.printable))\n",
    "    return new_word.strip(strip_str).lower()\n",
    "\n",
    "def clean_text_list(doc):\n",
    "    words = doc.split()\n",
    "    clean_words = [clean_word(word) for word in words]\n",
    "    return clean_words\n",
    "    \n",
    "def word_pairs(doc, window=3):\n",
    "    \"\"\"\n",
    "    Returns a list of 2-tuples, which are pairs of words where the second word in the tuple\n",
    "    appears within 'window' words of the first word.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc (str) : a string representation of the document\n",
    "    window (int) : how many words to the left and right should be considered 'nearby'\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    words = doc.split()\n",
    "    word_pairs = []\n",
    "    for i in range(len(words)):\n",
    "        word = clean_word(words[i])\n",
    "        index = i - window\n",
    "        end = i + window\n",
    "        while index <= end:\n",
    "            if index >= 0 and index < len(words) and index != i:\n",
    "                word_pairs.append((word, clean_word(words[index])))\n",
    "            index += 1\n",
    "    return word_pairs\n",
    "\n",
    "def nearby(doc, window=3):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of every word mapping to a set of the words within 'window' words \n",
    "    to either side. This is to say, if window = 3, then the 3 \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc (str) : a string representation of the document\n",
    "    window (int) : how many words to the left and right should be considered 'nearby'\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    words = doc.split()\n",
    "    nearby_words = defaultdict(set)\n",
    "    for i in range(len(words)):\n",
    "        word = clean_word(words[i])\n",
    "        index = i - window\n",
    "        end = i + window\n",
    "        while index <= end:\n",
    "            if index >= 0 and index < len(words) and index != i:\n",
    "                nearby_words[word].add(clean_word(words[index]))\n",
    "            index += 1\n",
    "    return nearby_words\n",
    "\n",
    "def vec_of_words(doc):\n",
    "    \"\"\"\n",
    "    Returns a numpy array of every distinct word in the given document.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc (str) : a string representation of the document\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    word_set = set((clean_word(word) for word in doc.split()))\n",
    "    return np.array(sorted(list(word_set)))\n",
    "\n",
    "def one_hot(word, all_words):\n",
    "    \"\"\"\n",
    "    Returns a one-hot numpy array of the position of 'word' in 'all_words'.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word (str) : the word for which to calculate the one-hot vector\n",
    "    all_words (numpy array) : vector containing all words in your dictionary\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    w = np.zeros(all_words.shape)\n",
    "    w[np.where(word_vec == word)] = 1\n",
    "    return w\n",
    "\n",
    "def output_vec(word, nearby, all_words):\n",
    "    \"\"\"\n",
    "    Returns the expected output vector for the given word. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word (str) : the word for which to calculate the output vector\n",
    "    nearby (dict) : dictionary of words mapping to their nearby words\n",
    "    all_words (numpy array) : vector containing all words in your dictionary\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    output = np.zeros(all_words.shape)\n",
    "    for w in nearby[word]:\n",
    "        value = 1 / len(nearby[word])\n",
    "        output[np.where(word_vec == w)] = value\n",
    "    return output\n",
    "\n",
    "def word_vec(word, nearby_pairs, all_words, features=300):\n",
    "    \"\"\"\n",
    "    Returns the word vector constructed by a neural network for the given word.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word (str) : the word for which to calculate the output vector\n",
    "    nearby_pairs (iterable) : contains all pairs where 'word' is the first element\n",
    "    all_words (numpy array) : vector containing all words in your dictionary\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(output_dim=features, input_dim=len(all_words)))\n",
    "    model.add(Dense(output_dim=len(all_words)))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='sgd')\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    word_pairs = [pair for pair in nearby_pairs if pair[0] == word]\n",
    "    for pair in word_pairs:\n",
    "        x.append(one_hot(word, all_words).reshape((561,)))\n",
    "        y.append(one_hot(pair[1], all_words))\n",
    "    \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    model.fit(x, y, batch_size=1, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = df['Text'].as_matrix()\n",
    "sentences = [clean_text_list(doc) for doc in text if type(doc) == str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences, size=300, window=5, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('your', 0.6379128098487854),\n",
       " ('myself', 0.5682736039161682),\n",
       " ('her', 0.5615739822387695),\n",
       " ('his', 0.5363878011703491),\n",
       " ('reformer', 0.5016362071037292),\n",
       " ('their', 0.48971113562583923),\n",
       " ('onos', 0.48773545026779175),\n",
       " ('i', 0.4750683307647705),\n",
       " ('mine', 0.4675109386444092),\n",
       " ('me', 0.4421350359916687)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['my'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs = [np.array([model.wv[word] for word in clean_text_list(doc) if word in model.wv]) \\\n",
    "        for doc in text if type(doc) == str]\n",
    "\n",
    "normal_docs = [doc / np.linalg.norm(doc,axis=1).reshape(-1,1) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_clusters(doc_number, num_clusters, doc_word_vectors, text_list, model):\n",
    "    document = doc_word_vectors[doc_number]\n",
    "    words = [word for word in clean_text_list(text_list[doc_number]) if word in model.wv]\n",
    "    word_vecs = {tuple(key): value for (key, value) in zip(document, words)}\n",
    "    \n",
    "    clusters = KMeans(n_clusters=num_clusters).fit(doc_word_vectors[doc_number])\n",
    "    \n",
    "    vector_clusters = defaultdict(set)\n",
    "    for i in range(len(clusters.labels_)):\n",
    "        label = clusters.labels_[i]\n",
    "        vector_clusters[label].add(tuple(doc_word_vectors[doc_number][i]))\n",
    "        \n",
    "    for cluster in vector_clusters.keys():\n",
    "        print(\"Cluster {}:\".format(cluster+1))\n",
    "        for vector in vector_clusters[cluster]:\n",
    "            print(word_vecs[vector])\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    return vector_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1:\n",
      "care\n",
      "asked\n",
      "love\n",
      "talk\n",
      "learned\n",
      "questions\n",
      "talking\n",
      "lesson\n",
      "thinking\n",
      "learn\n",
      "talks\n",
      "asking\n",
      "\n",
      "\n",
      "Cluster 2:\n",
      "biochemistry\n",
      "dealing\n",
      "anecdotes\n",
      "imitation\n",
      "modules\n",
      "voices\n",
      "subtitle\n",
      "artistic\n",
      "addressed\n",
      "philosophers\n",
      "formalized\n",
      "debates\n",
      "index\n",
      "sweeping\n",
      "synthesized\n",
      "fallacy\n",
      "encyclopedia\n",
      "collection\n",
      "cycle\n",
      "thermodynamics\n",
      "beginnings\n",
      "emergence\n",
      "imprint\n",
      "origins\n",
      "chairs\n",
      "demonstration\n",
      "cosmology\n",
      "memories\n",
      "associations\n",
      "selves\n",
      "compatible\n",
      "outlines\n",
      "narrating\n",
      "narrative\n",
      "myths\n",
      "creation\n",
      "sudden\n",
      "genesis\n",
      "geological\n",
      "formation\n",
      "\n",
      "\n",
      "Cluster 3:\n",
      "the\n",
      "\n",
      "\n",
      "Cluster 4:\n",
      "that\n",
      "which\n",
      "\n",
      "\n",
      "Cluster 5:\n",
      "of\n",
      "\n",
      "\n",
      "Cluster 6:\n",
      "i\n",
      "myself\n",
      "\n",
      "\n",
      "Cluster 7:\n",
      "to\n",
      "\n",
      "\n",
      "Cluster 8:\n",
      "isn't\n",
      "sounds\n",
      "that's\n",
      "its\n",
      "it's\n",
      "\n",
      "\n",
      "Cluster 9:\n",
      "deal\n",
      "view\n",
      "lot\n",
      "level\n",
      "sort\n",
      "idea\n",
      "part\n",
      "kind\n",
      "outside\n",
      "aspect\n",
      "great\n",
      "thought\n",
      "course\n",
      "\n",
      "\n",
      "Cluster 10:\n",
      "in\n",
      "\n",
      "\n",
      "Cluster 11:\n",
      "make\n",
      "watch\n",
      "keep\n",
      "give\n",
      "follow\n",
      "play\n",
      "divide\n",
      "look\n",
      "speak\n",
      "check\n",
      "help\n",
      "die\n",
      "offer\n",
      "live\n",
      "count\n",
      "take\n",
      "start\n",
      "get\n",
      "continue\n",
      "come\n",
      "go\n",
      "fit\n",
      "invent\n",
      "find\n",
      "bring\n",
      "pay\n",
      "carry\n",
      "lead\n",
      "store\n",
      "put\n",
      "\n",
      "\n",
      "Cluster 12:\n",
      "works\n",
      "looks\n",
      "is\n",
      "comes\n",
      "makes\n",
      "leads\n",
      "takes\n",
      "becomes\n",
      "\n",
      "\n",
      "Cluster 13:\n",
      "but\n",
      "\n",
      "\n",
      "Cluster 14:\n",
      "and\n",
      "\n",
      "\n",
      "Cluster 15:\n",
      "could\n",
      "let\n",
      "might\n",
      "may\n",
      "should\n",
      "would\n",
      "will\n",
      "did\n",
      "must\n",
      "needs\n",
      "does\n",
      "seems\n",
      "\n",
      "\n",
      "Cluster 16:\n",
      "it\n",
      "\n",
      "\n",
      "Cluster 17:\n",
      "wondering\n",
      "anyway\n",
      "silly\n",
      "sorry\n",
      "wasn't\n",
      "yeah\n",
      "funny\n",
      "oh\n",
      "mirsky:\n",
      "personally\n",
      "carroll:\n",
      "wow\n",
      "anymore\n",
      "what's\n",
      "i've\n",
      "okay\n",
      "\n",
      "\n",
      "Cluster 18:\n",
      "whatever\n",
      "whether\n",
      "what\n",
      "why\n",
      "\n",
      "\n",
      "Cluster 19:\n",
      "about\n",
      "\n",
      "\n",
      "Cluster 20:\n",
      "you\n",
      "\n",
      "\n",
      "Cluster 21:\n",
      "correct\n",
      "same\n",
      "going\n",
      "exactly\n",
      "either\n",
      "right\n",
      "trying\n",
      "supposed\n",
      "wrong\n",
      "\n",
      "\n",
      "Cluster 22:\n",
      "a\n",
      "\n",
      "\n",
      "Cluster 23:\n",
      "actually\n",
      "sometimes\n",
      "without\n",
      "probably\n",
      "literally\n",
      "weird\n",
      "obviously\n",
      "fine\n",
      "ever\n",
      "playing\n",
      "happy\n",
      "smart\n",
      "getting\n",
      "comfortable\n",
      "maybe\n",
      "little\n",
      "seeing\n",
      "saying\n",
      "otherwise\n",
      "dead\n",
      "bit\n",
      "yes\n",
      "always\n",
      "completely\n",
      "absolutely\n",
      "nobody\n",
      "felt\n",
      "happening\n",
      "else\n",
      "seriously\n",
      "bad\n",
      "sure\n",
      "chance\n",
      "never\n",
      "\n",
      "\n",
      "Cluster 24:\n",
      "told\n",
      "woman\n",
      "old\n",
      "recently\n",
      "interviewed\n",
      "fan\n",
      "met\n",
      "name\n",
      "spoke\n",
      "promised\n",
      "night\n",
      "house\n",
      "city\n",
      "he's\n",
      "visited\n",
      "guy\n",
      "movie\n",
      "family\n",
      "got\n",
      "dad\n",
      "husband\n",
      "invented\n",
      "imposter\n",
      "came\n",
      "enjoyed\n",
      "knew\n",
      "happened\n",
      "started\n",
      "went\n",
      "wonderful\n",
      "\n",
      "\n",
      "Cluster 25:\n",
      "separate\n",
      "two\n",
      "different\n",
      "range\n",
      "individual\n",
      "other\n",
      "new\n",
      "various\n",
      "certain\n",
      "similar\n",
      "within\n",
      "three\n",
      "\n",
      "\n",
      "Cluster 26:\n",
      "dark\n",
      "gravity\n",
      "experiment\n",
      "quantum\n",
      "mechanics\n",
      "consciousness\n",
      "existence\n",
      "itself\n",
      "universe\n",
      "bang\n",
      "mystery\n",
      "space\n",
      "planet\n",
      "basically\n",
      "\n",
      "\n",
      "Cluster 27:\n",
      "franz\n",
      "catches\n",
      "bells\n",
      "well-respected\n",
      "resolved\n",
      "bayes\n",
      "frivolous\n",
      "delighted\n",
      "admire\n",
      "descartes\n",
      "existed\n",
      "lonely\n",
      "[laughter]\n",
      "anecdote\n",
      "deciding\n",
      "naturalism\n",
      "objectively\n",
      "cleaning\n",
      "honestly\n",
      "wildly\n",
      "misunderstood\n",
      "refuse\n",
      "plausible\n",
      "respectable\n",
      "atp\n",
      "contemplating\n",
      "legitimate\n",
      "scratching\n",
      "immaterial\n",
      "regret\n",
      "cows\n",
      "conviction\n",
      "freaked\n",
      "weren't\n",
      "he'd\n",
      "avoided\n",
      "who's\n",
      "eternal\n",
      "courage\n",
      "reverend\n",
      "haven't\n",
      "inevitable\n",
      "humble\n",
      "defend\n",
      "ha\n",
      "laugh\n",
      "illegitimate\n",
      "complain\n",
      "escaped\n",
      "poetic\n",
      "hardcore\n",
      "eternity\n",
      "mirsky\n",
      "deborah\n",
      "time's\n",
      "naturalist\n",
      "picture:\n",
      "fluffy\n",
      "penguin\n",
      "sheep\n",
      "nuts\n",
      "me:\n",
      "searle\n",
      "complaining\n",
      "incomplete\n",
      "soul\n",
      "five-minute\n",
      "alarm\n",
      "[laughs]\n",
      "defensive\n",
      "backfire\n",
      "believing\n",
      "afterwards\n",
      "refused\n",
      "blind\n",
      "delusion\n",
      "affection\n",
      "retrospect\n",
      "\n",
      "\n",
      "Cluster 28:\n",
      "so\n",
      "\n",
      "\n",
      "Cluster 29:\n",
      "web\n",
      "thanks\n",
      "twitter\n",
      "site\n",
      "insights\n",
      "page\n",
      "print\n",
      "full\n",
      "quote\n",
      "news\n",
      "website\n",
      "files\n",
      "info\n",
      "plan\n",
      "buy\n",
      "2016\n",
      "free\n",
      "welcome\n",
      "version\n",
      "episode\n",
      "posted\n",
      "latest\n",
      "brief\n",
      "video\n",
      "channel\n",
      "feature\n",
      "weeks\n",
      "feed\n",
      "series\n",
      "atlantic\n",
      "puzzles\n",
      "update\n",
      "section\n",
      "description\n",
      "\n",
      "\n",
      "Cluster 30:\n",
      "noise\n",
      "forces\n",
      "terms\n",
      "rules\n",
      "molecules\n",
      "entropy\n",
      "laws\n",
      "properties\n",
      "inner\n",
      "interacting\n",
      "symbols\n",
      "functions\n",
      "particle\n",
      "atoms\n",
      "random\n",
      "particles\n",
      "elements\n",
      "tables\n",
      "probabilities\n",
      "perfectly\n",
      "tiny\n",
      "cells\n",
      "dense\n",
      "increasing\n",
      "amounts\n",
      "parts\n",
      "density\n",
      "solar\n",
      "cell\n",
      "energy\n",
      "dna\n",
      "\n",
      "\n",
      "Cluster 31:\n",
      "been\n",
      "being\n",
      "be\n",
      "become\n",
      "\n",
      "\n",
      "Cluster 32:\n",
      "logic\n",
      "modified\n",
      "analysis\n",
      "explore\n",
      "conventional\n",
      "essence\n",
      "physical\n",
      "describing\n",
      "biological\n",
      "engine\n",
      "chemical\n",
      "considering\n",
      "elegant\n",
      "search\n",
      "framework\n",
      "informal\n",
      "uses\n",
      "known\n",
      "quantitative\n",
      "artificial\n",
      "computer\n",
      "used\n",
      "bayesian\n",
      "fundamental\n",
      "cognitive\n",
      "inference\n",
      "complexity\n",
      "implicit\n",
      "logical\n",
      "specifically\n",
      "underlying\n",
      "enormous\n",
      "fundamentally\n",
      "special\n",
      "using\n",
      "prior\n",
      "statistical\n",
      "evolution\n",
      "called\n",
      "building\n",
      "\n",
      "\n",
      "Cluster 33:\n",
      "are\n",
      "were\n",
      "\n",
      "\n",
      "Cluster 34:\n",
      "\n",
      "\n",
      "\n",
      "Cluster 35:\n",
      "on\n",
      "\n",
      "\n",
      "Cluster 36:\n",
      "or\n",
      "\n",
      "\n",
      "Cluster 37:\n",
      "was\n",
      "\n",
      "\n",
      "Cluster 38:\n",
      "person\n",
      "moment\n",
      "truth\n",
      "reason\n",
      "difference\n",
      "choice\n",
      "situation\n",
      "question\n",
      "least\n",
      "mistake\n",
      "true\n",
      "case\n",
      "thing\n",
      "fact\n",
      "time\n",
      "argument\n",
      "answer\n",
      "one\n",
      "problem\n",
      "false\n",
      "connection\n",
      "\n",
      "\n",
      "Cluster 39:\n",
      "foundations\n",
      "physics\n",
      "elementary\n",
      "biology\n",
      "psychology\n",
      "theoretical\n",
      "chemistry\n",
      "statistics\n",
      "philosophy\n",
      "\n",
      "\n",
      "Cluster 40:\n",
      "big\n",
      "only\n",
      "just\n",
      "\n",
      "\n",
      "Cluster 41:\n",
      "this\n",
      "\n",
      "\n",
      "Cluster 42:\n",
      "at\n",
      "\n",
      "\n",
      "Cluster 43:\n",
      "subjects\n",
      "jobs\n",
      "issues\n",
      "information\n",
      "aspects\n",
      "stories\n",
      "matters\n",
      "data\n",
      "communities\n",
      "brains\n",
      "experiences\n",
      "animals\n",
      "perception\n",
      "beliefs\n",
      "biases\n",
      "own\n",
      "interests\n",
      "test\n",
      "beings\n",
      "members\n",
      "mental\n",
      "emotional\n",
      "computers\n",
      "topics\n",
      "organisms\n",
      "emotions\n",
      "desires\n",
      "act\n",
      "lives\n",
      "scientists\n",
      "trust\n",
      "models\n",
      "rely\n",
      "\n",
      "\n",
      "Cluster 44:\n",
      "with\n",
      "\n",
      "\n",
      "Cluster 45:\n",
      "consider\n",
      "change\n",
      "require\n",
      "mean\n",
      "accept\n",
      "say\n",
      "notice\n",
      "happen\n",
      "show\n",
      "believe\n",
      "call\n",
      "seem\n",
      "exist\n",
      "agree\n",
      "\n",
      "\n",
      "Cluster 46:\n",
      "these\n",
      "those\n",
      "\n",
      "\n",
      "Cluster 47:\n",
      "done\n",
      "do\n",
      "doing\n",
      "\n",
      "\n",
      "Cluster 48:\n",
      "game\n",
      "looking\n",
      "line\n",
      "track\n",
      "player\n",
      "cards\n",
      "opposite\n",
      "path\n",
      "hit\n",
      "starts\n",
      "starting\n",
      "open\n",
      "wrap\n",
      "room\n",
      "keeping\n",
      "whole\n",
      "inside\n",
      "moving\n",
      "end\n",
      "again\n",
      "points\n",
      "heads\n",
      "step\n",
      "goes\n",
      "point\n",
      "then\n",
      "set\n",
      "origin\n",
      "sitting\n",
      "turns\n",
      "putting\n",
      "place\n",
      "picture\n",
      "instead\n",
      "\n",
      "\n",
      "Cluster 49:\n",
      "some\n",
      "many\n",
      "few\n",
      "any\n",
      "most\n",
      "\n",
      "\n",
      "Cluster 50:\n",
      "gave\n",
      "brought\n",
      "taught\n",
      "found\n",
      "discovered\n",
      "described\n",
      "changed\n",
      "proposed\n",
      "explained\n",
      "made\n",
      "looked\n",
      "published\n",
      "mentioned\n",
      "revealed\n",
      "created\n",
      "showed\n",
      "recognized\n",
      "gained\n",
      "understood\n",
      "believed\n",
      "\n",
      "\n",
      "Cluster 51:\n",
      "book\n",
      "reading\n",
      "writing\n",
      "author\n",
      "story\n",
      "books\n",
      "papers\n",
      "title\n",
      "\n",
      "\n",
      "Cluster 52:\n",
      "us\n",
      "players\n",
      "someone\n",
      "me\n",
      "them\n",
      "people\n",
      "others\n",
      "physicists\n",
      "friends\n",
      "\n",
      "\n",
      "Cluster 53:\n",
      "an\n",
      "\n",
      "\n",
      "Cluster 54:\n",
      "whenever\n",
      "exercise\n",
      "credence\n",
      "item\n",
      "exact\n",
      "figure\n",
      "primitive\n",
      "exists\n",
      "electron\n",
      "indication\n",
      "limit\n",
      "instant\n",
      "likelihood\n",
      "irrational\n",
      "action\n",
      "probability\n",
      "small\n",
      "surface\n",
      "given\n",
      "analogy\n",
      "hypothesis\n",
      "plane\n",
      "group\n",
      "actual\n",
      "notion\n",
      "area\n",
      "above\n",
      "isolated\n",
      "proposition\n",
      "type\n",
      "appearance\n",
      "carries\n",
      "color\n",
      "non-zero\n",
      "meaning\n",
      "form\n",
      "atom\n",
      "arrow\n",
      "trivial\n",
      "number\n",
      "therefore\n",
      "length\n",
      "rational\n",
      "pair\n",
      "tree\n",
      "shown\n",
      "suit\n",
      "zero\n",
      "fraction\n",
      "amount\n",
      "multiverse\n",
      "single\n",
      "choosing\n",
      "singularity\n",
      "example:\n",
      "\n",
      "\n",
      "Cluster 55:\n",
      "both\n",
      "all\n",
      "\n",
      "\n",
      "Cluster 56:\n",
      "said\n",
      "now\n",
      "still\n",
      "says\n",
      "amazing\n",
      "also\n",
      "certainly\n",
      "everywhere\n",
      "\n",
      "\n",
      "Cluster 57:\n",
      "need\n",
      "ourselves\n",
      "try\n",
      "wanted\n",
      "use\n",
      "want\n",
      "able\n",
      "attempt\n",
      "willing\n",
      "\n",
      "\n",
      "Cluster 58:\n",
      "who\n",
      "\n",
      "\n",
      "Cluster 59:\n",
      "down\n",
      "around\n",
      "eventually\n",
      "out\n",
      "away\n",
      "through\n",
      "up\n",
      "together\n",
      "along\n",
      "back\n",
      "into\n",
      "off\n",
      "\n",
      "\n",
      "Cluster 60:\n",
      "by\n",
      "under\n",
      "\n",
      "\n",
      "Cluster 61:\n",
      "not\n",
      "\n",
      "\n",
      "Cluster 62:\n",
      "there\n",
      "\n",
      "\n",
      "Cluster 63:\n",
      "had\n",
      "has\n",
      "have\n",
      "having\n",
      "we've\n",
      "\n",
      "\n",
      "Cluster 64:\n",
      "if\n",
      "before\n",
      "where\n",
      "when\n",
      "\n",
      "\n",
      "Cluster 65:\n",
      "she\n",
      "he\n",
      "him\n",
      "\n",
      "\n",
      "Cluster 66:\n",
      "they\n",
      "\n",
      "\n",
      "Cluster 67:\n",
      "even\n",
      "than\n",
      "\n",
      "\n",
      "Cluster 68:\n",
      "much\n",
      "such\n",
      "well\n",
      "\n",
      "\n",
      "Cluster 69:\n",
      "her\n",
      "his\n",
      "my\n",
      "\n",
      "\n",
      "Cluster 70:\n",
      "we\n",
      "\n",
      "\n",
      "Cluster 71:\n",
      "as\n",
      "\n",
      "\n",
      "Cluster 72:\n",
      "moon\n",
      "near\n",
      "hits\n",
      "falls\n",
      "beach\n",
      "tv\n",
      "grains\n",
      "star\n",
      "passes\n",
      "sand\n",
      "forth\n",
      "hot\n",
      "band\n",
      "ground\n",
      "egg\n",
      "runs\n",
      "ocean\n",
      "sun\n",
      "glass\n",
      "arm\n",
      "clocks\n",
      "running\n",
      "wall\n",
      "\n",
      "\n",
      "Cluster 73:\n",
      "really\n",
      "something\n",
      "stuff\n",
      "anything\n",
      "nothing\n",
      "things\n",
      "everything\n",
      "\n",
      "\n",
      "Cluster 74:\n",
      "against\n",
      "almost\n",
      "10\n",
      "high\n",
      "ten\n",
      "passed\n",
      "credit\n",
      "average\n",
      "states\n",
      "costs\n",
      "11\n",
      "hillary\n",
      "trump\n",
      "clinton\n",
      "times\n",
      "billions\n",
      "years\n",
      "12\n",
      "compared\n",
      "ago\n",
      "party\n",
      "million\n",
      "after\n",
      "over\n",
      "age\n",
      "among\n",
      "counted\n",
      "paid\n",
      "since\n",
      "thousand\n",
      "100\n",
      "percent\n",
      "couple\n",
      "until\n",
      "lost\n",
      "\n",
      "\n",
      "Cluster 75:\n",
      "intelligence\n",
      "deep\n",
      "understanding\n",
      "brain\n",
      "sense\n",
      "ultimately\n",
      "learning\n",
      "theory\n",
      "detail\n",
      "process\n",
      "field\n",
      "task\n",
      "role\n",
      "language\n",
      "potential\n",
      "reality\n",
      "evidence\n",
      "nature\n",
      "human\n",
      "decision\n",
      "body\n",
      "response\n",
      "effect\n",
      "belief\n",
      "theories\n",
      "context\n",
      "discovery\n",
      "background\n",
      "system\n",
      "ideas\n",
      "purpose\n",
      "environment\n",
      "basis\n",
      "prediction\n",
      "\n",
      "\n",
      "Cluster 76:\n",
      "like\n",
      "\n",
      "\n",
      "Cluster 77:\n",
      "history\n",
      "attention\n",
      "life\n",
      "experience\n",
      "work\n",
      "\n",
      "\n",
      "Cluster 78:\n",
      "underwater\n",
      "broadly\n",
      "reactions\n",
      "optimized\n",
      "favorable\n",
      "contracting\n",
      "mimic\n",
      "pushing\n",
      "alien\n",
      "explanatory\n",
      "ease\n",
      "conscious\n",
      "samples\n",
      "carbon\n",
      "dioxide\n",
      "metabolism\n",
      "crowd\n",
      "barrier\n",
      "replication\n",
      "genetic\n",
      "rna\n",
      "gauge\n",
      "fibers\n",
      "criticism\n",
      "inorganic\n",
      "convictions\n",
      "molecule\n",
      "proteins\n",
      "ambitious\n",
      "updating\n",
      "instructions\n",
      "accordingly\n",
      "biologists\n",
      "slot\n",
      "resist\n",
      "animal\n",
      "gather\n",
      "uncover\n",
      "pinpointing\n",
      "examining\n",
      "strongly\n",
      "towards\n",
      "inevitably\n",
      "heuristics\n",
      "mechanisms\n",
      "damage\n",
      "attack\n",
      "muscles\n",
      "muscle\n",
      "intentionality\n",
      "trace\n",
      "unconscious\n",
      "churning\n",
      "macroscopic\n",
      "aspirations\n",
      "label\n",
      "sustained\n",
      "external\n",
      "propagated\n",
      "uncertainties\n",
      "uncertain\n",
      "\n",
      "\n",
      "Cluster 79:\n",
      "their\n",
      "our\n",
      "\n",
      "\n",
      "Cluster 80:\n",
      "your\n",
      "\n",
      "\n",
      "Cluster 81:\n",
      "no\n",
      "\n",
      "\n",
      "Cluster 82:\n",
      "can\n",
      "\n",
      "\n",
      "Cluster 83:\n",
      "previous\n",
      "best\n",
      "every\n",
      "next\n",
      "beginning\n",
      "another\n",
      "last\n",
      "first\n",
      "entire\n",
      "\n",
      "\n",
      "Cluster 84:\n",
      "we're\n",
      "they're\n",
      "am\n",
      "i'm\n",
      "happens\n",
      "you're\n",
      "\n",
      "\n",
      "Cluster 85:\n",
      "conference\n",
      "leaders\n",
      "minority\n",
      "generations\n",
      "debate\n",
      "business\n",
      "according\n",
      "expert\n",
      "chinese\n",
      "report\n",
      "tech\n",
      "popular\n",
      "scientific\n",
      "research\n",
      "offices\n",
      "science\n",
      "foundation\n",
      "held\n",
      "defense\n",
      "early\n",
      "program\n",
      "\n",
      "\n",
      "Cluster 86:\n",
      "alan\n",
      "forest\n",
      "prize\n",
      "clara\n",
      "93\n",
      "columbia\n",
      "1989\n",
      "publisher\n",
      "famous\n",
      "jet\n",
      "propulsion\n",
      "laboratory\n",
      "champion\n",
      "david\n",
      "hume\n",
      "named\n",
      "thomas\n",
      "nobel\n",
      "stanford\n",
      "chris\n",
      "de\n",
      "hans\n",
      "philosopher\n",
      "laureate\n",
      "et\n",
      "jane\n",
      "editor\n",
      "donald\n",
      "skinner\n",
      "sidney\n",
      "jpl\n",
      "m\n",
      "carroll\n",
      "steve\n",
      "cal\n",
      "physicist\n",
      "sean\n",
      "lancaster\n",
      "york\n",
      "9th\n",
      "john\n",
      "turing\n",
      "winner\n",
      "matthews\n",
      "president\n",
      "basketball\n",
      "notices\n",
      "resort\n",
      "parliament\n",
      "54\n",
      "fellow\n",
      "bozeman\n",
      "mike\n",
      "russell\n",
      "scientist\n",
      "\n",
      "\n",
      "Cluster 87:\n",
      "challenging\n",
      "good\n",
      "familiar\n",
      "clear\n",
      "reasonable\n",
      "truly\n",
      "necessarily\n",
      "common\n",
      "subtle\n",
      "helpful\n",
      "examples\n",
      "interesting\n",
      "clearly\n",
      "useful\n",
      "somewhat\n",
      "obvious\n",
      "here\n",
      "impossible\n",
      "simple\n",
      "exciting\n",
      "beautiful\n",
      "incredibly\n",
      "important\n",
      "possible\n",
      "example\n",
      "easy\n",
      "complicated\n",
      "hard\n",
      "likely\n",
      "crucial\n",
      "\n",
      "\n",
      "Cluster 88:\n",
      "remember\n",
      "explain\n",
      "discuss\n",
      "remind\n",
      "imagine\n",
      "hope\n",
      "hear\n",
      "think\n",
      "tell\n",
      "recognize\n",
      "know\n",
      "feel\n",
      "ask\n",
      "see\n",
      "describe\n",
      "understand\n",
      "\n",
      "\n",
      "Cluster 89:\n",
      "though\n",
      "because\n",
      "\n",
      "\n",
      "Cluster 90:\n",
      "complex\n",
      "between\n",
      "real\n",
      "natural\n",
      "\n",
      "\n",
      "Cluster 91:\n",
      "how\n",
      "\n",
      "\n",
      "Cluster 92:\n",
      "doesn't\n",
      "couldn't\n",
      "didn't\n",
      "don't\n",
      "shouldn't\n",
      "let's\n",
      "can't\n",
      "anthropomorphic\n",
      "you'd\n",
      "you'll\n",
      "\n",
      "\n",
      "Cluster 93:\n",
      "world\n",
      "\n",
      "\n",
      "Cluster 94:\n",
      "matter\n",
      "gets\n",
      "knows\n",
      "there's\n",
      "\n",
      "\n",
      "Cluster 95:\n",
      "worker\n",
      "waste\n",
      "brush\n",
      "momentarily\n",
      "box\n",
      "carrying\n",
      "51\n",
      "tweet\n",
      "clicking\n",
      "floating\n",
      "gorilla\n",
      "spots\n",
      "whose\n",
      "illness\n",
      "green\n",
      "heavier\n",
      "cheese\n",
      "milk\n",
      "nut\n",
      "chicken\n",
      "stores\n",
      "sweep\n",
      "poker\n",
      "traced\n",
      "contract\n",
      "lift\n",
      "wine\n",
      "photon\n",
      "red\n",
      "\n",
      "\n",
      "Cluster 96:\n",
      "too\n",
      "enough\n",
      "very\n",
      "pretty\n",
      "\n",
      "\n",
      "Cluster 97:\n",
      "trickier\n",
      "harder\n",
      "worse\n",
      "more\n",
      "easier\n",
      "better\n",
      "rather\n",
      "higher\n",
      "\n",
      "\n",
      "Cluster 98:\n",
      "for\n",
      "\n",
      "\n",
      "Cluster 99:\n",
      "from\n",
      "\n",
      "\n",
      "Cluster 100:\n",
      "ways\n",
      "way\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = get_clusters(3, 100, normal_docs, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_ = KMeans()\n",
    "clusters = model_.fit(normal_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vector_clusters = defaultdict(set)\n",
    "for i in range(len(clusters.labels_)):\n",
    "    label = clusters.labels_[i]\n",
    "    vector_clusters[label].add(tuple(normal_docs[0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cluster in vector_clusters.keys():\n",
    "    print(\"Cluster {}:\".format(cluster+1))\n",
    "    for vector in vector_clusters[cluster]:\n",
    "        print(word_vecs[vector])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5809\n"
     ]
    }
   ],
   "source": [
    "print(len(clean_text_list(text[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
