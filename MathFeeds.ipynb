{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\kaden\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from lxml import etree\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, core\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import gensim\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xml2df(xml_data):\n",
    "    tree = ET.parse(xml_data)\n",
    "    root = tree.getroot()\n",
    "    all_records = []\n",
    "    headers = []\n",
    "    for i, child in enumerate(root):\n",
    "        record = []\n",
    "        for subchild in child:\n",
    "            record.append(subchild.text)\n",
    "            if subchild.tag not in headers:\n",
    "                headers.append(subchild.tag)\n",
    "        all_records.append(record)\n",
    "    return pd.DataFrame(all_records, columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = xml2df(\"MathFeedsDataAll.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Domain</th>\n",
       "      <th>blurb</th>\n",
       "      <th>date</th>\n",
       "      <th>image</th>\n",
       "      <th>isbn</th>\n",
       "      <th>kicker</th>\n",
       "      <th>price</th>\n",
       "      <th>timesDeleted</th>\n",
       "      <th>timesEmailed</th>\n",
       "      <th>timesOpened</th>\n",
       "      <th>timesSaved</th>\n",
       "      <th>timesShared</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>wordtitle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://blogs.ams.org/mathgradblog/2017/01/05/d...</td>\n",
       "      <td>Up to Date Blog Content for JMM 2017</td>\n",
       "      <td>Looking for blog content about the 2017 Joint ...</td>\n",
       "      <td>blogs.ams.org</td>\n",
       "      <td>Looking for blog content about the 2017 Joint ...</td>\n",
       "      <td>01/05/17</td>\n",
       "      <td>None</td>\n",
       "      <td>nan</td>\n",
       "      <td>GRADUATE STUDENT BLOG</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>4.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>5774489532.046694</td>\n",
       "      <td>uptodateblogcontentforjmm2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mrhonner.com/archives/17215</td>\n",
       "      <td>Math Photo: A Dodecagon of Octagons « Mr Honner</td>\n",
       "      <td>I’d never looked closely at the Parachute Jump...</td>\n",
       "      <td>mrhonner.com</td>\n",
       "      <td>I'd never looked closely at the Parachute Jump...</td>\n",
       "      <td>09/18/16</td>\n",
       "      <td>http://MrHonner.com/wp-content/uploads/2016/09...</td>\n",
       "      <td>nan</td>\n",
       "      <td>NOTABLE</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>5783496448.673001</td>\n",
       "      <td>mathphotoadodecagonofoctagons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://mathbabe.org/2017/03/21/guest-post-the...</td>\n",
       "      <td>Guest post: the age of algorithms | mathbabe</td>\n",
       "      <td>Artie has kindly allowed me to post his though...</td>\n",
       "      <td>mathbabe.org</td>\n",
       "      <td>Artie has kindly allowed me to post his though...</td>\n",
       "      <td>03/21/17</td>\n",
       "      <td>None</td>\n",
       "      <td>nan</td>\n",
       "      <td>BLOG</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>5767987618.7198305</td>\n",
       "      <td>guestposttheageofalgorithms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.scientificamerican.com/podcast/epis...</td>\n",
       "      <td>Sean M. Carroll Looks at The Big Picture - Sci...</td>\n",
       "      <td>Steve Mirsky: Welcome to Scientific American's...</td>\n",
       "      <td>scientificamerican.com</td>\n",
       "      <td>Caltech theoretical physicist Sean M. Carroll ...</td>\n",
       "      <td>05/12/16</td>\n",
       "      <td>https://www.scientificamerican.com/sciam/cache...</td>\n",
       "      <td>nan</td>\n",
       "      <td>None</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>5795107200.0</td>\n",
       "      <td>seanmcarrolllooksatthebigpicture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://the-japan-news.com/news/article/0003176002</td>\n",
       "      <td>The Japan News</td>\n",
       "      <td>Not found\\n\\nThe requested server cannot be ac...</td>\n",
       "      <td>the-japan-news.com</td>\n",
       "      <td>The education ministry will open research cent...</td>\n",
       "      <td>09/13/16</td>\n",
       "      <td>http://the-japan-news.com/modules/img/logo_ogp...</td>\n",
       "      <td>nan</td>\n",
       "      <td>BIG DATA</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>5784392943.842338</td>\n",
       "      <td>educationministrytopromoteuseofbigdata</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL  \\\n",
       "0  http://blogs.ams.org/mathgradblog/2017/01/05/d...   \n",
       "1                 http://mrhonner.com/archives/17215   \n",
       "2  https://mathbabe.org/2017/03/21/guest-post-the...   \n",
       "3  http://www.scientificamerican.com/podcast/epis...   \n",
       "4  http://the-japan-news.com/news/article/0003176002   \n",
       "\n",
       "                                               Title  \\\n",
       "0               Up to Date Blog Content for JMM 2017   \n",
       "1    Math Photo: A Dodecagon of Octagons « Mr Honner   \n",
       "2       Guest post: the age of algorithms | mathbabe   \n",
       "3  Sean M. Carroll Looks at The Big Picture - Sci...   \n",
       "4                                     The Japan News   \n",
       "\n",
       "                                                Text                  Domain  \\\n",
       "0  Looking for blog content about the 2017 Joint ...           blogs.ams.org   \n",
       "1  I’d never looked closely at the Parachute Jump...            mrhonner.com   \n",
       "2  Artie has kindly allowed me to post his though...            mathbabe.org   \n",
       "3  Steve Mirsky: Welcome to Scientific American's...  scientificamerican.com   \n",
       "4  Not found\\n\\nThe requested server cannot be ac...      the-japan-news.com   \n",
       "\n",
       "                                               blurb      date  \\\n",
       "0  Looking for blog content about the 2017 Joint ...  01/05/17   \n",
       "1  I'd never looked closely at the Parachute Jump...  09/18/16   \n",
       "2  Artie has kindly allowed me to post his though...  03/21/17   \n",
       "3  Caltech theoretical physicist Sean M. Carroll ...  05/12/16   \n",
       "4  The education ministry will open research cent...  09/13/16   \n",
       "\n",
       "                                               image isbn  \\\n",
       "0                                               None  nan   \n",
       "1  http://MrHonner.com/wp-content/uploads/2016/09...  nan   \n",
       "2                                               None  nan   \n",
       "3  https://www.scientificamerican.com/sciam/cache...  nan   \n",
       "4  http://the-japan-news.com/modules/img/logo_ogp...  nan   \n",
       "\n",
       "                  kicker price timesDeleted timesEmailed timesOpened  \\\n",
       "0  GRADUATE STUDENT BLOG   nan          1.0          nan         4.0   \n",
       "1                NOTABLE   nan          nan          nan         nan   \n",
       "2                   BLOG   nan          2.0          2.0        11.0   \n",
       "3                   None   nan          nan          nan         nan   \n",
       "4               BIG DATA   nan          nan          nan         nan   \n",
       "\n",
       "  timesSaved timesShared           timestamp  \\\n",
       "0        nan         nan   5774489532.046694   \n",
       "1        nan         nan   5783496448.673001   \n",
       "2        1.0         nan  5767987618.7198305   \n",
       "3        nan         nan        5795107200.0   \n",
       "4        nan         nan   5784392943.842338   \n",
       "\n",
       "                                wordtitle  \n",
       "0           uptodateblogcontentforjmm2017  \n",
       "1           mathphotoadodecagonofoctagons  \n",
       "2             guestposttheageofalgorithms  \n",
       "3        seanmcarrolllooksatthebigpicture  \n",
       "4  educationministrytopromoteuseofbigdata  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "#df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_word(w):\n",
    "    strip_str = \"()\\\".?!,;\"\n",
    "    new_word = \"\".join((c for c in w if c in string.printable))\n",
    "    return new_word.strip(strip_str).lower()\n",
    "\n",
    "def clean_text_list(doc):\n",
    "    words = doc.split()\n",
    "    clean_words = [clean_word(word) for word in words]\n",
    "    return clean_words\n",
    "    \n",
    "def word_pairs(doc, window=3):\n",
    "    \"\"\"\n",
    "    Returns a list of 2-tuples, which are pairs of words where the second word in the tuple\n",
    "    appears within 'window' words of the first word.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc (str) : a string representation of the document\n",
    "    window (int) : how many words to the left and right should be considered 'nearby'\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    words = doc.split()\n",
    "    word_pairs = []\n",
    "    for i in range(len(words)):\n",
    "        word = clean_word(words[i])\n",
    "        index = i - window\n",
    "        end = i + window\n",
    "        while index <= end:\n",
    "            if index >= 0 and index < len(words) and index != i:\n",
    "                word_pairs.append((word, clean_word(words[index])))\n",
    "            index += 1\n",
    "    return word_pairs\n",
    "\n",
    "def nearby(doc, window=3):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of every word mapping to a set of the words within 'window' words \n",
    "    to either side. This is to say, if window = 3, then the 3 \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc (str) : a string representation of the document\n",
    "    window (int) : how many words to the left and right should be considered 'nearby'\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    words = doc.split()\n",
    "    nearby_words = defaultdict(set)\n",
    "    for i in range(len(words)):\n",
    "        word = clean_word(words[i])\n",
    "        index = i - window\n",
    "        end = i + window\n",
    "        while index <= end:\n",
    "            if index >= 0 and index < len(words) and index != i:\n",
    "                nearby_words[word].add(clean_word(words[index]))\n",
    "            index += 1\n",
    "    return nearby_words\n",
    "\n",
    "def vec_of_words(doc):\n",
    "    \"\"\"\n",
    "    Returns a numpy array of every distinct word in the given document.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc (str) : a string representation of the document\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    word_set = set((clean_word(word) for word in doc.split()))\n",
    "    return np.array(sorted(list(word_set)))\n",
    "\n",
    "def one_hot(word, all_words):\n",
    "    \"\"\"\n",
    "    Returns a one-hot numpy array of the position of 'word' in 'all_words'.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word (str) : the word for which to calculate the one-hot vector\n",
    "    all_words (numpy array) : vector containing all words in your dictionary\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    w = np.zeros(all_words.shape)\n",
    "    w[np.where(word_vec == word)] = 1\n",
    "    return w\n",
    "\n",
    "def output_vec(word, nearby, all_words):\n",
    "    \"\"\"\n",
    "    Returns the expected output vector for the given word. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word (str) : the word for which to calculate the output vector\n",
    "    nearby (dict) : dictionary of words mapping to their nearby words\n",
    "    all_words (numpy array) : vector containing all words in your dictionary\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    output = np.zeros(all_words.shape)\n",
    "    for w in nearby[word]:\n",
    "        value = 1 / len(nearby[word])\n",
    "        output[np.where(word_vec == w)] = value\n",
    "    return output\n",
    "\n",
    "def word_vec(word, nearby_pairs, all_words, features=300):\n",
    "    \"\"\"\n",
    "    Returns the word vector constructed by a neural network for the given word.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word (str) : the word for which to calculate the output vector\n",
    "    nearby_pairs (iterable) : contains all pairs where 'word' is the first element\n",
    "    all_words (numpy array) : vector containing all words in your dictionary\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(output_dim=features, input_dim=len(all_words)))\n",
    "    model.add(Dense(output_dim=len(all_words)))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='sgd')\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    word_pairs = [pair for pair in nearby_pairs if pair[0] == word]\n",
    "    for pair in word_pairs:\n",
    "        x.append(one_hot(word, all_words).reshape((561,)))\n",
    "        y.append(one_hot(pair[1], all_words))\n",
    "    \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    model.fit(x, y, batch_size=1, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = df['Text'].as_matrix()\n",
    "sentences = [clean_text_list(doc) for doc in text if type(doc) == str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences, size=300, window=5, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('your', 0.6222735643386841),\n",
       " ('her', 0.5724108815193176),\n",
       " ('myself', 0.5565168261528015),\n",
       " ('his', 0.5364807844161987),\n",
       " ('mine', 0.5186669826507568),\n",
       " ('their', 0.5103070735931396),\n",
       " ('i', 0.4740857481956482),\n",
       " ('onos', 0.44803839921951294),\n",
       " ('apeirophobia', 0.4475870728492737),\n",
       " ('disdained', 0.43811196088790894)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['my'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = [np.array([model.wv[word] for word in clean_text_list(doc) if word in model.wv]) \\\n",
    "        for doc in text if type(doc) == str]\n",
    "\n",
    "normal_docs = [doc / np.linalg.norm(doc,axis=1).reshape(-1,1) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_clusters(doc_number, num_clusters, doc_word_vectors, text_list, model):\n",
    "    document = doc_word_vectors[doc_number]\n",
    "    words = [word for word in clean_text_list(text_list[doc_number]) if word in model.wv]\n",
    "    word_vecs = {tuple(key): value for (key, value) in zip(document, words)}\n",
    "    \n",
    "    clusters = KMeans(n_clusters=num_clusters).fit(doc_word_vectors[doc_number])\n",
    "    \n",
    "    vector_clusters = defaultdict(set)\n",
    "    for i in range(len(clusters.labels_)):\n",
    "        label = clusters.labels_[i]\n",
    "        vector_clusters[label].add(tuple(doc_word_vectors[doc_number][i]))\n",
    "        \n",
    "    for cluster in vector_clusters.keys():\n",
    "        print(\"Cluster {}:\".format(cluster+1))\n",
    "        for vector in vector_clusters[cluster]:\n",
    "            print(word_vecs[vector])\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    return vector_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = get_clusters(3, 100, normal_docs, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_ = KMeans()\n",
    "clusters = model_.fit(normal_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_clusters = defaultdict(set)\n",
    "for i in range(len(clusters.labels_)):\n",
    "    label = clusters.labels_[i]\n",
    "    vector_clusters[label].add(tuple(normal_docs[0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for cluster in vector_clusters.keys():\n",
    "    print(\"Cluster {}:\".format(cluster+1))\n",
    "    for vector in vector_clusters[cluster]:\n",
    "        print(word_vecs[vector])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(clean_text_list(text[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
