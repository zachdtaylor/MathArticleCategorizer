{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from lxml import etree\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, core\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import gensim\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xml2df(xml_data):\n",
    "    tree = ET.parse(xml_data)\n",
    "    root = tree.getroot()\n",
    "    all_records = []\n",
    "    headers = []\n",
    "    for i, child in enumerate(root):\n",
    "        record = []\n",
    "        for subchild in child:\n",
    "            record.append(subchild.text)\n",
    "            if subchild.tag not in headers:\n",
    "                headers.append(subchild.tag)\n",
    "        all_records.append(record)\n",
    "    return pd.DataFrame(all_records, columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = xml2df(\"MathFeedsDataAll.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Domain</th>\n",
       "      <th>blurb</th>\n",
       "      <th>date</th>\n",
       "      <th>image</th>\n",
       "      <th>isbn</th>\n",
       "      <th>kicker</th>\n",
       "      <th>price</th>\n",
       "      <th>timesDeleted</th>\n",
       "      <th>timesEmailed</th>\n",
       "      <th>timesOpened</th>\n",
       "      <th>timesSaved</th>\n",
       "      <th>timesShared</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>wordtitle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://blogs.ams.org/mathgradblog/2017/01/05/d...</td>\n",
       "      <td>Up to Date Blog Content for JMM 2017</td>\n",
       "      <td>Looking for blog content about the 2017 Joint ...</td>\n",
       "      <td>blogs.ams.org</td>\n",
       "      <td>Looking for blog content about the 2017 Joint ...</td>\n",
       "      <td>01/05/17</td>\n",
       "      <td>None</td>\n",
       "      <td>nan</td>\n",
       "      <td>GRADUATE STUDENT BLOG</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>4.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>5774489532.046694</td>\n",
       "      <td>uptodateblogcontentforjmm2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mrhonner.com/archives/17215</td>\n",
       "      <td>Math Photo: A Dodecagon of Octagons « Mr Honner</td>\n",
       "      <td>I’d never looked closely at the Parachute Jump...</td>\n",
       "      <td>mrhonner.com</td>\n",
       "      <td>I'd never looked closely at the Parachute Jump...</td>\n",
       "      <td>09/18/16</td>\n",
       "      <td>http://MrHonner.com/wp-content/uploads/2016/09...</td>\n",
       "      <td>nan</td>\n",
       "      <td>NOTABLE</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>5783496448.673001</td>\n",
       "      <td>mathphotoadodecagonofoctagons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://mathbabe.org/2017/03/21/guest-post-the...</td>\n",
       "      <td>Guest post: the age of algorithms | mathbabe</td>\n",
       "      <td>Artie has kindly allowed me to post his though...</td>\n",
       "      <td>mathbabe.org</td>\n",
       "      <td>Artie has kindly allowed me to post his though...</td>\n",
       "      <td>03/21/17</td>\n",
       "      <td>None</td>\n",
       "      <td>nan</td>\n",
       "      <td>BLOG</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>5767987618.7198305</td>\n",
       "      <td>guestposttheageofalgorithms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.scientificamerican.com/podcast/epis...</td>\n",
       "      <td>Sean M. Carroll Looks at The Big Picture - Sci...</td>\n",
       "      <td>Steve Mirsky: Welcome to Scientific American's...</td>\n",
       "      <td>scientificamerican.com</td>\n",
       "      <td>Caltech theoretical physicist Sean M. Carroll ...</td>\n",
       "      <td>05/12/16</td>\n",
       "      <td>https://www.scientificamerican.com/sciam/cache...</td>\n",
       "      <td>nan</td>\n",
       "      <td>None</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>5795107200.0</td>\n",
       "      <td>seanmcarrolllooksatthebigpicture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://the-japan-news.com/news/article/0003176002</td>\n",
       "      <td>The Japan News</td>\n",
       "      <td>Not found\\n\\nThe requested server cannot be ac...</td>\n",
       "      <td>the-japan-news.com</td>\n",
       "      <td>The education ministry will open research cent...</td>\n",
       "      <td>09/13/16</td>\n",
       "      <td>http://the-japan-news.com/modules/img/logo_ogp...</td>\n",
       "      <td>nan</td>\n",
       "      <td>BIG DATA</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>5784392943.842338</td>\n",
       "      <td>educationministrytopromoteuseofbigdata</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL  \\\n",
       "0  http://blogs.ams.org/mathgradblog/2017/01/05/d...   \n",
       "1                 http://mrhonner.com/archives/17215   \n",
       "2  https://mathbabe.org/2017/03/21/guest-post-the...   \n",
       "3  http://www.scientificamerican.com/podcast/epis...   \n",
       "4  http://the-japan-news.com/news/article/0003176002   \n",
       "\n",
       "                                               Title  \\\n",
       "0               Up to Date Blog Content for JMM 2017   \n",
       "1    Math Photo: A Dodecagon of Octagons « Mr Honner   \n",
       "2       Guest post: the age of algorithms | mathbabe   \n",
       "3  Sean M. Carroll Looks at The Big Picture - Sci...   \n",
       "4                                     The Japan News   \n",
       "\n",
       "                                                Text                  Domain  \\\n",
       "0  Looking for blog content about the 2017 Joint ...           blogs.ams.org   \n",
       "1  I’d never looked closely at the Parachute Jump...            mrhonner.com   \n",
       "2  Artie has kindly allowed me to post his though...            mathbabe.org   \n",
       "3  Steve Mirsky: Welcome to Scientific American's...  scientificamerican.com   \n",
       "4  Not found\\n\\nThe requested server cannot be ac...      the-japan-news.com   \n",
       "\n",
       "                                               blurb      date  \\\n",
       "0  Looking for blog content about the 2017 Joint ...  01/05/17   \n",
       "1  I'd never looked closely at the Parachute Jump...  09/18/16   \n",
       "2  Artie has kindly allowed me to post his though...  03/21/17   \n",
       "3  Caltech theoretical physicist Sean M. Carroll ...  05/12/16   \n",
       "4  The education ministry will open research cent...  09/13/16   \n",
       "\n",
       "                                               image isbn  \\\n",
       "0                                               None  nan   \n",
       "1  http://MrHonner.com/wp-content/uploads/2016/09...  nan   \n",
       "2                                               None  nan   \n",
       "3  https://www.scientificamerican.com/sciam/cache...  nan   \n",
       "4  http://the-japan-news.com/modules/img/logo_ogp...  nan   \n",
       "\n",
       "                  kicker price timesDeleted timesEmailed timesOpened  \\\n",
       "0  GRADUATE STUDENT BLOG   nan          1.0          nan         4.0   \n",
       "1                NOTABLE   nan          nan          nan         nan   \n",
       "2                   BLOG   nan          2.0          2.0        11.0   \n",
       "3                   None   nan          nan          nan         nan   \n",
       "4               BIG DATA   nan          nan          nan         nan   \n",
       "\n",
       "  timesSaved timesShared           timestamp  \\\n",
       "0        nan         nan   5774489532.046694   \n",
       "1        nan         nan   5783496448.673001   \n",
       "2        1.0         nan  5767987618.7198305   \n",
       "3        nan         nan        5795107200.0   \n",
       "4        nan         nan   5784392943.842338   \n",
       "\n",
       "                                wordtitle  \n",
       "0           uptodateblogcontentforjmm2017  \n",
       "1           mathphotoadodecagonofoctagons  \n",
       "2             guestposttheageofalgorithms  \n",
       "3        seanmcarrolllooksatthebigpicture  \n",
       "4  educationministrytopromoteuseofbigdata  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "#df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_word(w):\n",
    "    strip_str = \"()\\\".?!,;\"\n",
    "    new_word = \"\".join((c for c in w if c in string.printable))\n",
    "    return new_word.strip(strip_str).lower()\n",
    "\n",
    "def clean_text_list(doc):\n",
    "    words = doc.split()\n",
    "    clean_words = [clean_word(word) for word in words]\n",
    "    return clean_words\n",
    "    \n",
    "def word_pairs(doc, window=3):\n",
    "    \"\"\"\n",
    "    Returns a list of 2-tuples, which are pairs of words where the second word in the tuple\n",
    "    appears within 'window' words of the first word.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc (str) : a string representation of the document\n",
    "    window (int) : how many words to the left and right should be considered 'nearby'\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    words = doc.split()\n",
    "    word_pairs = []\n",
    "    for i in range(len(words)):\n",
    "        word = clean_word(words[i])\n",
    "        index = i - window\n",
    "        end = i + window\n",
    "        while index <= end:\n",
    "            if index >= 0 and index < len(words) and index != i:\n",
    "                word_pairs.append((word, clean_word(words[index])))\n",
    "            index += 1\n",
    "    return word_pairs\n",
    "\n",
    "def nearby(doc, window=3):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of every word mapping to a set of the words within 'window' words \n",
    "    to either side. This is to say, if window = 3, then the 3 \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc (str) : a string representation of the document\n",
    "    window (int) : how many words to the left and right should be considered 'nearby'\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    words = doc.split()\n",
    "    nearby_words = defaultdict(set)\n",
    "    for i in range(len(words)):\n",
    "        word = clean_word(words[i])\n",
    "        index = i - window\n",
    "        end = i + window\n",
    "        while index <= end:\n",
    "            if index >= 0 and index < len(words) and index != i:\n",
    "                nearby_words[word].add(clean_word(words[index]))\n",
    "            index += 1\n",
    "    return nearby_words\n",
    "\n",
    "def vec_of_words(doc):\n",
    "    \"\"\"\n",
    "    Returns a numpy array of every distinct word in the given document.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc (str) : a string representation of the document\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    word_set = set((clean_word(word) for word in doc.split()))\n",
    "    return np.array(sorted(list(word_set)))\n",
    "\n",
    "def one_hot(word, all_words):\n",
    "    \"\"\"\n",
    "    Returns a one-hot numpy array of the position of 'word' in 'all_words'.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word (str) : the word for which to calculate the one-hot vector\n",
    "    all_words (numpy array) : vector containing all words in your dictionary\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    w = np.zeros(all_words.shape)\n",
    "    w[np.where(word_vec == word)] = 1\n",
    "    return w\n",
    "\n",
    "def output_vec(word, nearby, all_words):\n",
    "    \"\"\"\n",
    "    Returns the expected output vector for the given word. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word (str) : the word for which to calculate the output vector\n",
    "    nearby (dict) : dictionary of words mapping to their nearby words\n",
    "    all_words (numpy array) : vector containing all words in your dictionary\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    output = np.zeros(all_words.shape)\n",
    "    for w in nearby[word]:\n",
    "        value = 1 / len(nearby[word])\n",
    "        output[np.where(word_vec == w)] = value\n",
    "    return output\n",
    "\n",
    "def word_vec(word, nearby_pairs, all_words, features=300):\n",
    "    \"\"\"\n",
    "    Returns the word vector constructed by a neural network for the given word.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word (str) : the word for which to calculate the output vector\n",
    "    nearby_pairs (iterable) : contains all pairs where 'word' is the first element\n",
    "    all_words (numpy array) : vector containing all words in your dictionary\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(output_dim=features, input_dim=len(all_words)))\n",
    "    model.add(Dense(output_dim=len(all_words)))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='sgd')\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    word_pairs = [pair for pair in nearby_pairs if pair[0] == word]\n",
    "    for pair in word_pairs:\n",
    "        x.append(one_hot(word, all_words).reshape((561,)))\n",
    "        y.append(one_hot(pair[1], all_words))\n",
    "    \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    model.fit(x, y, batch_size=1, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = df['Text'].as_matrix()\n",
    "sentences = [clean_text_list(doc) for doc in text if type(doc) == str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model = gensim.models.Word2Vec(sentences, size=300, window=5, min_count=5)\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('My', 0.7721933126449585),\n",
       " ('I', 0.7249468564987183),\n",
       " ('myself', 0.6657763719558716),\n",
       " ('me', 0.6557403802871704),\n",
       " ('your', 0.6517343521118164),\n",
       " ('inmy', 0.6101812124252319),\n",
       " ('his', 0.6074063777923584),\n",
       " ('her', 0.5852575302124023),\n",
       " ('onmy', 0.5586036443710327),\n",
       " ('yours', 0.5553128719329834)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['my'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = [np.array([model.wv[word] for word in clean_text_list(doc) if word in model.wv]) \\\n",
    "        for doc in text if type(doc) == str]\n",
    "\n",
    "normal_docs = [doc / np.linalg.norm(doc,axis=1).reshape(-1,1) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_clusters(doc_number, num_clusters, doc_word_vectors, text_list, model):\n",
    "    document = doc_word_vectors[doc_number]\n",
    "    words = [word for word in clean_text_list(text_list[doc_number]) if word in model.wv]\n",
    "    word_vecs = {tuple(key): value for (key, value) in zip(document, words)}\n",
    "    \n",
    "    clusters = KMeans(n_clusters=num_clusters).fit(doc_word_vectors[doc_number])\n",
    "    \n",
    "    vector_clusters = defaultdict(set)\n",
    "    for i in range(len(clusters.labels_)):\n",
    "        label = clusters.labels_[i]\n",
    "        vector_clusters[label].add(tuple(doc_word_vectors[doc_number][i]))\n",
    "        \n",
    "    for cluster in vector_clusters.keys():\n",
    "        print(\"Cluster {}:\".format(cluster+1))\n",
    "        for vector in vector_clusters[cluster]:\n",
    "            print(word_vecs[vector])\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    return vector_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1:\n",
      "systems\n",
      "process\n",
      "system\n",
      "\n",
      "\n",
      "Cluster 2:\n",
      "this\n",
      "the\n",
      "\n",
      "\n",
      "Cluster 3:\n",
      "most\n",
      "among\n",
      "few\n",
      "other\n",
      "mostly\n",
      "certain\n",
      "least\n",
      "several\n",
      "all\n",
      "two\n",
      "those\n",
      "some\n",
      "one\n",
      "\n",
      "\n",
      "Cluster 4:\n",
      "able\n",
      "might\n",
      "can\n",
      "cannot\n",
      "wanted\n",
      "likely\n",
      "should\n",
      "would\n",
      "may\n",
      "must\n",
      "will\n",
      "does\n",
      "could\n",
      "\n",
      "\n",
      "Cluster 5:\n",
      "outside\n",
      "in\n",
      "where\n",
      "\n",
      "\n",
      "Cluster 6:\n",
      "testers\n",
      "tests\n",
      "tested\n",
      "testing\n",
      "test\n",
      "\n",
      "\n",
      "Cluster 7:\n",
      "was\n",
      "seems\n",
      "is\n",
      "represents\n",
      "remains\n",
      "\n",
      "\n",
      "Cluster 8:\n",
      "an\n",
      "another\n",
      "\n",
      "\n",
      "Cluster 9:\n",
      "simple\n",
      "solution\n",
      "rfp\n",
      "scenarios\n",
      "concurrently\n",
      "component\n",
      "incoming\n",
      "correctly\n",
      "configurable\n",
      "interface\n",
      "design\n",
      "types\n",
      "encode\n",
      "protocols\n",
      "livestream\n",
      "interconnection\n",
      "email\n",
      "reliably\n",
      "application\n",
      "function\n",
      "intuitively\n",
      "arbitrary\n",
      "inputs\n",
      "responsive\n",
      "developers\n",
      "code\n",
      "coders\n",
      "designers\n",
      "specifiers\n",
      "accuracy\n",
      "software\n",
      "solutions\n",
      "\n",
      "\n",
      "Cluster 10:\n",
      "organization\n",
      "procuring\n",
      "purchasers\n",
      "relationships\n",
      "thriving\n",
      "service\n",
      "management\n",
      "efforts\n",
      "party\n",
      "procurer\n",
      "purchasing\n",
      "areas\n",
      "product\n",
      "operations\n",
      "network\n",
      "telco\n",
      "business\n",
      "patents\n",
      "engineering\n",
      "its\n",
      "location\n",
      "activities\n",
      "outsourced\n",
      "outsourcing\n",
      "organizations\n",
      "demand\n",
      "marketing\n",
      "ceo\n",
      "financial\n",
      "company\n",
      "contracts\n",
      "developer\n",
      "\n",
      "\n",
      "Cluster 11:\n",
      "watson\n",
      "arthur\n",
      "bs\n",
      "my\n",
      "ms\n",
      "dr\n",
      "julia\n",
      "i\n",
      "february\n",
      "continua\n",
      "etc\n",
      "ibm\n",
      "\n",
      "\n",
      "Cluster 12:\n",
      "answers\n",
      "wonders\n",
      "question\n",
      "discussion\n",
      "thoughts\n",
      "conversation\n",
      "asked\n",
      "matter\n",
      "dear\n",
      "distinctions\n",
      "issue\n",
      "issues\n",
      "ask\n",
      "questions\n",
      "\n",
      "\n",
      "Cluster 13:\n",
      "effectiveness\n",
      "plausibility\n",
      "potential\n",
      "whether\n",
      "possibility\n",
      "legality\n",
      "extent\n",
      "efficacy\n",
      "\n",
      "\n",
      "Cluster 14:\n",
      "or\n",
      "individual\n",
      "your\n",
      "either\n",
      "any\n",
      "\n",
      "\n",
      "Cluster 15:\n",
      "way\n",
      "ways\n",
      "how\n",
      "what\n",
      "\n",
      "\n",
      "Cluster 16:\n",
      "return\n",
      "set\n",
      "cover\n",
      "alternative\n",
      "for\n",
      "offers\n",
      "offer\n",
      "support\n",
      "offering\n",
      "seek\n",
      "advance\n",
      "free\n",
      "\n",
      "\n",
      "Cluster 17:\n",
      "performance\n",
      "work\n",
      "guest\n",
      "take\n",
      "operate\n",
      "perform\n",
      "live\n",
      "conduct\n",
      "conducted\n",
      "performing\n",
      "undergo\n",
      "\n",
      "\n",
      "Cluster 18:\n",
      "be\n",
      "being\n",
      "\n",
      "\n",
      "Cluster 19:\n",
      "by\n",
      "\n",
      "\n",
      "Cluster 20:\n",
      "described\n",
      "stated\n",
      "reported\n",
      "promoted\n",
      "observed\n",
      "called\n",
      "specify\n",
      "released\n",
      "investigated\n",
      "named\n",
      "similar\n",
      "brief\n",
      "regarding\n",
      "discussed\n",
      "interested\n",
      "addressed\n",
      "considering\n",
      "proposed\n",
      "shared\n",
      "negotiated\n",
      "\n",
      "\n",
      "Cluster 21:\n",
      "were\n",
      "these\n",
      "are\n",
      "ones\n",
      "\n",
      "\n",
      "Cluster 22:\n",
      "caught\n",
      "obvious\n",
      "sign\n",
      "stark\n",
      "watching\n",
      "no\n",
      "there\n",
      "pointed\n",
      "goes\n",
      "hear\n",
      "ready\n",
      "willing\n",
      "\n",
      "\n",
      "Cluster 23:\n",
      "it\n",
      "much\n",
      "someone\n",
      "me\n",
      "more\n",
      "he\n",
      "\n",
      "\n",
      "Cluster 24:\n",
      "destruction\n",
      "recidivism\n",
      "criminal\n",
      "recidivists\n",
      "crime\n",
      "perpetrators\n",
      "enforcement\n",
      "security\n",
      "reformed\n",
      "parole\n",
      "society\n",
      "convicts\n",
      "urban\n",
      "recidivist\n",
      "criminals\n",
      "solvability\n",
      "ferret\n",
      "weapons\n",
      "\n",
      "\n",
      "Cluster 25:\n",
      "developing\n",
      "auditing\n",
      "psychological\n",
      "competency\n",
      "selection\n",
      "measures\n",
      "investigation\n",
      "validating\n",
      "implementing\n",
      "improvement\n",
      "assessment\n",
      "prospective\n",
      "evaluation\n",
      "validated\n",
      "thorough\n",
      "development\n",
      "implementation\n",
      "evaluative\n",
      "\n",
      "\n",
      "Cluster 26:\n",
      "mechanized\n",
      "algorithmic\n",
      "mathematical\n",
      "marketers\n",
      "human\n",
      "priori\n",
      "neural\n",
      "creativity\n",
      "proprietary\n",
      "inherent\n",
      "\n",
      "\n",
      "Cluster 27:\n",
      "neither\n",
      "nor\n",
      "mean\n",
      "again\n",
      "whatever\n",
      "simply\n",
      "easily\n",
      "actually\n",
      "try\n",
      "although\n",
      "but\n",
      "if\n",
      "they\n",
      "not\n",
      "then\n",
      "do\n",
      "only\n",
      "even\n",
      "however\n",
      "\n",
      "\n",
      "Cluster 28:\n",
      "with\n",
      "while\n",
      "\n",
      "\n",
      "Cluster 29:\n",
      "best\n",
      "worst\n",
      "leading\n",
      "professional\n",
      "better\n",
      "good\n",
      "oriented\n",
      "hard\n",
      "largest\n",
      "reliable\n",
      "consistent\n",
      "invaluable\n",
      "team\n",
      "disappointing\n",
      "\n",
      "\n",
      "Cluster 30:\n",
      "hiring\n",
      "staff\n",
      "members\n",
      "working\n",
      "executives\n",
      "volunteers\n",
      "employees\n",
      "employee\n",
      "employer\n",
      "\n",
      "\n",
      "Cluster 31:\n",
      "use\n",
      "sold\n",
      "used\n",
      "effective\n",
      "using\n",
      "designed\n",
      "purpose\n",
      "sequestered\n",
      "targeted\n",
      "\n",
      "\n",
      "Cluster 32:\n",
      "tutor\n",
      "teachers\n",
      "math\n",
      "learning\n",
      "teacher\n",
      "coursework\n",
      "degree\n",
      "class\n",
      "\n",
      "\n",
      "Cluster 33:\n",
      "claims\n",
      "case\n",
      "cases\n",
      "filings\n",
      "\n",
      "\n",
      "Cluster 34:\n",
      "algorithm\n",
      "methodology\n",
      "methods\n",
      "techniques\n",
      "engine\n",
      "algorithms\n",
      "heuristic\n",
      "\n",
      "\n",
      "Cluster 35:\n",
      "which\n",
      "that\n",
      "also\n",
      "\n",
      "\n",
      "Cluster 36:\n",
      "at\n",
      "\n",
      "\n",
      "Cluster 37:\n",
      "possible\n",
      "such\n",
      "as\n",
      "well\n",
      "beyond\n",
      "\n",
      "\n",
      "Cluster 38:\n",
      "definition\n",
      "requiring\n",
      "requirements\n",
      "means\n",
      "problematic\n",
      "appropriate\n",
      "allow\n",
      "facilitate\n",
      "defined\n",
      "difficult\n",
      "replaces\n",
      "allowed\n",
      "demanding\n",
      "requires\n",
      "need\n",
      "guarantee\n",
      "required\n",
      "needs\n",
      "retains\n",
      "require\n",
      "\n",
      "\n",
      "Cluster 39:\n",
      "objectives\n",
      "results\n",
      "\n",
      "\n",
      "Cluster 40:\n",
      "extensive\n",
      "analytical\n",
      "underlying\n",
      "characteristics\n",
      "qualitative\n",
      "correlation\n",
      "quantitative\n",
      "understanding\n",
      "detail\n",
      "data\n",
      "detailed\n",
      "research\n",
      "broad\n",
      "analysis\n",
      "modeling\n",
      "expert\n",
      "knowledge\n",
      "\n",
      "\n",
      "Cluster 41:\n",
      "check\n",
      "select\n",
      "predict\n",
      "identify\n",
      "improve\n",
      "demonstrate\n",
      "formulate\n",
      "devise\n",
      "assessed\n",
      "determined\n",
      "describe\n",
      "devising\n",
      "evaluate\n",
      "learn\n",
      "assess\n",
      "trainers\n",
      "depending\n",
      "examined\n",
      "\n",
      "\n",
      "Cluster 42:\n",
      "their\n",
      "others\n",
      "them\n",
      "his\n",
      "themselves\n",
      "who\n",
      "\n",
      "\n",
      "Cluster 43:\n",
      "decisions\n",
      "decision\n",
      "\n",
      "\n",
      "Cluster 44:\n",
      "on\n",
      "\n",
      "\n",
      "Cluster 45:\n",
      "candidate\n",
      "reliance\n",
      "bias\n",
      "absolving\n",
      "consequences\n",
      "arising\n",
      "liability\n",
      "scrutiny\n",
      "name\n",
      "sanity\n",
      "side\n",
      "drag\n",
      "leave\n",
      "shadow\n",
      "position\n",
      "risk\n",
      "fundamentals\n",
      "cautious\n",
      "further\n",
      "ethical\n",
      "part\n",
      "life\n",
      "post\n",
      "ill\n",
      "pleasure\n",
      "retired\n",
      "mistake\n",
      "credit\n",
      "stead\n",
      "situation\n",
      "respect\n",
      "opportunity\n",
      "effect\n",
      "advantage\n",
      "blame\n",
      "ethics\n",
      "task\n",
      "responsibility\n",
      "correct\n",
      "\n",
      "\n",
      "Cluster 46:\n",
      "make\n",
      "made\n",
      "\n",
      "\n",
      "Cluster 47:\n",
      "estimated\n",
      "percentage\n",
      "skewed\n",
      "proportion\n",
      "estimate\n",
      "point\n",
      "same\n",
      "predicts\n",
      "prior\n",
      "after\n",
      "period\n",
      "1\n",
      "2\n",
      "points\n",
      "weightings\n",
      "drop\n",
      "matched\n",
      "currently\n",
      "spent\n",
      "last\n",
      "basis\n",
      "worse\n",
      "month\n",
      "black\n",
      "share\n",
      "single\n",
      "than\n",
      "rates\n",
      "range\n",
      "ranging\n",
      "from\n",
      "nets\n",
      "higher\n",
      "before\n",
      "current\n",
      "compared\n",
      "first\n",
      "second\n",
      "\n",
      "\n",
      "Cluster 48:\n",
      "out\n",
      "box\n",
      "off\n",
      "\n",
      "\n",
      "Cluster 49:\n",
      "been\n",
      "have\n",
      "has\n",
      "years\n",
      "had\n",
      "\n",
      "\n",
      "Cluster 50:\n",
      "disinterested\n",
      "thinks\n",
      "interesting\n",
      "always\n",
      "example\n",
      "essentially\n",
      "perhaps\n",
      "seem\n",
      "suggesting\n",
      "attractive\n",
      "sounding\n",
      "extreme\n",
      "retrospect\n",
      "arguable\n",
      "very\n",
      "manner\n",
      "heavily\n",
      "characteristic\n",
      "emblematic\n",
      "obviously\n",
      "technically\n",
      "large\n",
      "kindly\n",
      "regard\n",
      "assume\n",
      "thoughtful\n",
      "particular\n",
      "imagine\n",
      "equally\n",
      "course\n",
      "significant\n",
      "certainly\n",
      "thing\n",
      "personally\n",
      "notoriously\n",
      "basically\n",
      "change\n",
      "particularly\n",
      "think\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = get_clusters(2, 50, normal_docs, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_ = KMeans()\n",
    "clusters = model_.fit(normal_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_clusters = defaultdict(set)\n",
    "for i in range(len(clusters.labels_)):\n",
    "    label = clusters.labels_[i]\n",
    "    vector_clusters[label].add(tuple(normal_docs[0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for cluster in vector_clusters.keys():\n",
    "    print(\"Cluster {}:\".format(cluster+1))\n",
    "    for vector in vector_clusters[cluster]:\n",
    "        print(word_vecs[vector])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(clean_text_list(text[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Looking for blog content about the 2017 Joint Mathematics Meetings? Check out the JMM 2017 Blog where you can catch up on yesterday’s main events and keep up with some of the main events during the meeting. I really enjoyed reading Profession, State of since I couldn’t attend the full panel discussion but I can still hear the ideas for increasing diversity in the professions from the awesome set of panelists.\\n\\nSome of the posts are done live so attendees can jump in on events while they happen. Other posts are done after the event which allows the authors to give more of an overview of everything that occurred.\\n\\n0 0 14'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
